# -*- coding: utf-8 -*-
"""BDA Assingment 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xlW8ZX4OjIpPEsfs5RHLsHIR9vwrf2HC

# BDA Assignment 2

## Preparation
"""

#open the working directory by attaching a google drive
from google.colab import drive
drive.mount('/content/drive')

#installing jdk, spark and pyspark 
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz
!pip install -q findspark
!pip install pyspark

#setting up environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"

#setting up pyspark and other prerequisites
import findspark
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
findspark.init()
from pyspark.sql import SparkSession
#creating the spark session
spark = SparkSession.builder.master("local[*]").getOrCreate()
# verify that Spark is running
print("The created SparkSession object:", spark)
print("Spark version:",spark.version)

"""## Part I
### 1. Load the Dataframe
"""

file_path = "/content/drive/MyDrive/BDA_A2/Medical_info.csv"
file_schema = "id LONG, age INT, BMI DOUBLE, PSA DOUBLE, TG DOUBLE,  Cholesterol DOUBLE, LDLChole DOUBLE, HDLChole DOUBLE, Glucose DOUBLE, Testosterone DOUBLE , BP_1 INT"

#loading the dataframe from the csv file
df = spark.read.csv(file_path, header=True, schema=file_schema)

#printing the actually loaded schema
df.printSchema()
#showing the first 20 rows of the loaded spark dataframe
df.show()
#calculating the total number of observations(rows) in the dataframe
print("Total number of rows:", df.count())

"""The Medical_info DataFrame has a shape of (6967,11) So it has the id collumn, 10 feature collumns (age, BMI, PSA, TG, Cholesterol, LDLChole, HDLChole, Glucose, Testosterone, BP_1).
It consists of 6967 rows (observations).

### 2. Create a new DataFrame without the null/missing values
"""

#count before drop
print("Count of rows: ", df.count())
#replacing the zeroes with numpy NaN objects
df = df.replace(0, np.nan)
#dropping the rows which have a NaN
df_dropped = df.dropna()
#count after drop
print("Shape of the dataframe after the drop: ", df_dropped.count())

#calculating number of dropped rows:
print("All in all", df.count()-df_dropped.count(), "rows were dropped.\nThere are", df_dropped.count(), "rows left in the dataframe.")

"""### 3. Summary statistics of the 'age' feature, histogram for the 'age' feature

"""

#converting to pandas dataframe
pd_df = df_dropped.toPandas()
#printing statistics of the 'age' column
print(pd_df["age"].describe())
#calculating the variance of the 'age' feature
print("Variance of age column ", pd_df["age"].var())

# plotting the histogram of the age 
pd_df.hist(column = "age", bins=18)

plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of "age" column')

"""### 4. Display the quartile Information of the "BMI" feature"""

#printing the statistics f the 'BMI' column
print(pd_df["BMI"].describe())

#generating the boxplot
pd_df.boxplot("BMI", showmeans=True, figsize=(10,10))
#annotating the plot
plt.ylabel('Value')
plt.title('BMI Boxplot')

pd_df['BMI'].hist(bins=20)

"""### 5. Use Spark DataFrame API to count the number of rows where 'age' is greater than 50 and 'BP_1' equals 1"""

import pyspark.sql.functions as f

filter_50_BP1 = df_dropped.filter((f.col("age")>50) & (f.col("BP_1")==1))

print("There are", filter_50_BP1.count(), "rows where age > 50 and BP_1 equals 1.")

"""### 6. Build to classification models with 'BP_1' as target label and evaluate the performance

#### A
"""

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import VectorAssembler

# transforming BP_1 to 0 for low and 1 for high Blood Pressure
df_dropped = df_dropped.withColumn('label', (df_dropped.BP_1== 2).cast('integer'))

#set up of the VectorAssembler
VecAss = VectorAssembler(inputCols=[
 'age', 'BMI', 'PSA', 'TG', 'Cholesterol', 'LDLChole', 'HDLChole', 'Glucose', 'Testosterone'
], outputCol='features')

#assembling features
assembled_df = VecAss.transform(df_dropped)

#printing the first ten rows pf only predictor features and outcome column
assembled_df.select("features", "label").show(10, truncate=False)

#Splitting the data
data_train, data_test = assembled_df.randomSplit([0.7, 0.3], seed=42)

#creating and running the decision tree classifier
dtc = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth=3, maxBins=16).fit(data_train)

#testing the trained classifier
dtc_prediction = dtc.transform(data_test)
dtc_prediction.select("features", "label", "prediction").show(10, truncate=False)

# GBT classification
from pyspark.ml.classification import GBTClassifier
gbt = GBTClassifier(labelCol='label', maxDepth=5, maxBins=16, maxIter=10).fit(data_train)

# testing the trained classifier
gbt_prediction = gbt.transform(data_test)
gbt_prediction.select("features", "label", "prediction").show(10, truncate=False)

from pyspark.ml.evaluation import BinaryClassificationEvaluator

BinEval = BinaryClassificationEvaluator()

# scores of the two classifiers
print("Score for Decision Tree Classifier", BinEval.evaluate(dtc_prediction))
print("Score for Gradient Boost Tree", BinEval.evaluate(gbt_prediction))

import seaborn as sns

# Confusion Matrices
# Decision Tree Classifier

df_dtc = dtc_prediction.toPandas()
conf_mat_dtc = pd.crosstab(df_dtc['label'], df_dtc['prediction'], rownames=['Actual'], colnames=['Predicted'])

sns.heatmap(conf_mat_dtc, annot=True, fmt=".0f", cmap='YlGnBu')
plt.show()

#needs to be rewritten... don't know how to get numbers of heatmap yet.
# True Negative
TN = dtc_prediction.filter('prediction = 0 AND label = prediction').count()
# True Positive
TP = dtc_prediction.filter('prediction = 1 AND label = prediction').count()
# False Negative 
FN = dtc_prediction.filter('prediction = 0 AND label != prediction').count()
# False Positive
FP = dtc_prediction.filter('prediction = 1 AND label != prediction').count()
 
# The accuracy is the ratio of correct predictions (TP and TN) to all predictions (TP, TN, FP and FN)
accuracy = (TN + TP) / (TN + TP + FN + FP)
 
#precise
precise = TP/(TP+TN)
 
#recall
recall = TP/(TP + FN)
print("Decision Tree Model\nAccuracy: {:.2f}\nPrecision:  {:.2f}\nRecall:   {:.2f}"\
      .format(accuracy, precise, recall))

# Confusion Matrices
# Gradient Boost Tree Classifier

df_gbt = gbt_prediction.toPandas()
conf_mat_gbt = pd.crosstab(df_gbt['label'], df_gbt['prediction'], rownames=['Actual'], colnames=['Predicted'])

sns.heatmap(conf_mat_gbt, annot=True, fmt=".0f", cmap='YlGnBu')
plt.show()

#needs to be rewritten... don't know how to get numbers of heatmap yet.
# True Negative
TN = gbt_prediction.filter('prediction = 0 AND label = prediction').count()
# True Positive
TP = gbt_prediction.filter('prediction = 1 AND label = prediction').count()
# False Negative 
FN = gbt_prediction.filter('prediction = 0 AND label != prediction').count()
# False Positive
FP = gbt_prediction.filter('prediction = 1 AND label != prediction').count()
 
# The accuracy is the ratio of correct predictions (TP and TN) to all predictions (TP, TN, FP and FN)
accuracy = (TN + TP) / (TN + TP + FN + FP)
 
#precise
precise = TP/(TP+TN)
 
#recall
recall = TP/(TP + FN)
print("Gradient Boost Tree Model\nAccuracy: {:.2f}\nPrecision:  {:.2f}\nRecall:   {:.2f}"\
      .format(accuracy, precise, recall))

"""## Part II
### 1. Load the Dataframe
"""

file_path = "/content/drive/MyDrive/BDA_A2/Region_info.csv"
df = spark.read.csv(file_path, header=True, schema= "population DOUBLE, fertility DOUBLE, HIV DOUBLE, CO2 DOUBLE, BMI_male DOUBLE, GDP DOUBLE, BMI_female DOUBLE, life DOUBLE, child_mortality DOUBLE, region STRING")

df.printSchema()
df.show(truncate=False)
print("Total number of rows:", df.count())

"""### 2. Create the second dataframe with 'region' column dropped"""

df_dropped = df.drop("region")
df_dropped.show(truncate=False)

"""### 3. Visualising the relationship betweeen 'fertility' and 'life' features"""

#lineplot to get an idea about the data
df_dropped.select('fertility', 'life').toPandas().plot()

#scatterplot so see the tendency, that the lower the fertility, the higher the life expectancy
plt.scatter(df_dropped.select('fertility').toPandas(), df_dropped.select('life').toPandas(), alpha=0.5)
plt.xlabel('fertility')
plt.ylabel('life expectancy')
plt.title('Relationship between fertility and life expectancy')

"""### 4. Using Spark SQL query the dtataframe where fertility is greater than 1.0 and life is greater than 70"""

# Create a temporary table 
df_dropped.createOrReplaceTempView("region")
#building the sql query
query = "SELECT * FROM region WHERE fertility>1.0 AND life>70"

#running the sql query
flt = spark.sql(query)

print("There are", flt.count(), "rows where fertility > 1.0 and life > 70.")

flt.show()

"""### 5. Building a Linear Regression Model to predict life expectancy based on fertility """

#preparation for linear regression, feature assembly
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

#assembling the feature vectors
VecAss = VectorAssembler(inputCols=['fertility'], outputCol='features') 
df_assembled = VecAss.transform(df_dropped)

#setting up the linear regressor
lr = LinearRegression(featuresCol='features', labelCol='life',maxIter=10)

#splitting the data
(df_train, df_test) = df_assembled.randomSplit([0.8, 0.2])

#training the model
lr_model = lr.fit(df_train)

#checking the stats. with a standard deviation of around 9.12 the RMSE of 5.82 is quite good.
trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

df_dropped.select(['life']).describe().show()

#evaluation using the testing data and calculating the RMSE
test_result = lr_model.evaluate(df_test)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)

#making predictions and evaluating
lr_predictions = lr_model.transform(df_test)
lr_predictions.select("prediction","life","features").show(5)
from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="life",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))

"""### 6. Lasso Regression for 'life' using all other columns as predictor"""

from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
#preparing the data
VecAss = VectorAssembler(inputCols=['population', 'fertility', 'HIV', 
                                    'CO2', 'BMI_male', 'GDP', 
                                    'BMI_female', 'child_mortality'], outputCol='features') 
df_assembled = VecAss.transform(df_dropped)

#preparing the regressor (elasticNetParam = 1 for Lasso Regression)
lr = LinearRegression(featuresCol='features', labelCol='life',maxIter=10, elasticNetParam=1, regParam=0.1)
#splitting the data
(df_train, df_test) = df_assembled.randomSplit([0.7, 0.3])
#training the model
lr_model = lr.fit(df_train)

#checking the stats. with a standard deviation of around 9.01 the RMSE of 3.01 is very good.
trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

df_train.select(['life']).describe().show()

#making predictions and evaluating
lr_predictions = lr_model.transform(df_test)
lr_predictions.select("prediction","life","features").show(10, truncate=False)
test_result = lr_model.evaluate(df_test)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)
from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="life",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))

